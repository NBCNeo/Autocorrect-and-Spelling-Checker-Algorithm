{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrect and Spelling Checker\n",
    "#### Batsal Ghimire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Spelling checker is a program that identifies when a word is misspelled and provides the closest suggestion from the initialized dictionary. Spelling checkers are used in many applications as an integrated feature, most commonly seen in search engines, word processors, email clients, etc. It's also common to see spelling checkers in large databases to make it easier to find the correct entries. Spelling check feature is a must for most modern apps since it makes the user workflow much faster and satisfying. However, many large corporations use spell checkers that are very complex and are trained using large amounts of data, which might not be available for a regular user. So, we will be implementing a version of a spell checker that is functional and understandable to a wide range of people.\n",
    "\n",
    "### Goals\n",
    "* To have a spelling checker that takes in misspelled words or sentences as the input and gives out the corrected version of it as an output.\n",
    "* Has a reasonable and acceptable time complexity, memory complexity and accuracy.\n",
    "* Uses the probability of the occurrence of the word to choose between conflicting choices.\n",
    "* Can at least process the dictionary with a few thousand words in a reasonable time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to solving the problem\n",
    "* First, we will read all the correct words that are contained in the initially given dictionary.\n",
    "* Then, we will compare the input words to the known list of correct words.\n",
    "* Finally, we will find the closest word to the inputted word from the dictionary and return it to the user.\n",
    "\n",
    "This is the higher-level approach to solve this problem and will remain consistent throughout all detailed approaches. However, first we need to appreciate the challenges with this problem and understand that these algorithms will not provide the perfect result everytime.\n",
    "For example: if we misspell the word 'work' as 'worke', there will be no way to distinguish as to whether the word is 'worked' or 'work'. To solve this, we will use probability of occurrence of the words, that we will discuss later.\n",
    "\n",
    "### Implementations\n",
    "In this project, I will be looking at two main implementations, and among these two main categories, I will also explore further improvements and changes that can be made. Furthermore, I will not begin with a brute force approach to the problem i.e. compare each letter of the inputted word, to each of the  words in the dictionary. This will substantially increase the time complexity and with the dictionary size I will be using, this is simply not feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426016\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "f = open(\"tyo.txt\",\"r\") #This is a text extracted from Gutenberg which will act as a test case.\n",
    "text = f.read() #This will read the given text file.\n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75880\n"
     ]
    }
   ],
   "source": [
    "g = open(\"wordlist.txt\",\"r\") #This is a second test file consisting of around 75000 English words.\n",
    "texts = g.read() #Reads the text file.\n",
    "print (len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the files, we will need to filter out the strings. Why? Firstly, in Python, the capitalized and small versions of the same letter are not considered to be equal. Moreover, since we will constantly be comparing many strings together, filtering the words will make out job much more manageable. We will also remove any numbers since autocorrecting the number is not possible. After this, we will have a list of words that consists of 26 small English alphabets.\n",
    "\n",
    "I will be using a string manipulation package in this case to make the input more flexible. And, since I will be using the same filtering function for both the dictionary and the misspelled words, there might be numbers involved in it. I could have used the $txt.lower()$ to get lowercase letters and stripped the spaces with $txt.strip()$, but to make things more efficient, I will be using the 're' package that has certain built-in capabilities I will be making use of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filtered_words(text):\n",
    "    return re.findall('[a-z]+',text.lower()) #Returns the list of lowercase words from the given text. Removes all numbers and spaces as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be justifying the use of a few modules that I will be using. The mmh3 will enable us to create hash functions very quickly. It takes in two arguments i.e. the element that is to be hashed and the seeds. The random module will be used to generate pseudo-random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First I will import all the required packages.\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import mmh3\n",
    "import hashlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Bloom Filters\n",
    "Counting Bloom Filters are a special type of a data structure that can tell us whether an item is present in the given set or not. This data structure has an excellent constant time complexity as $O(k)$, where 'k' is the number of hash functions. They are also very memory efficient since only minimal data is stored. The price we pay for these efficiencies is that the data structure is probabilistic and there is a risk of getting a false positive. However, we can alter the value of the false positive rate by changing the memory size that is allocated. The optimal number of hash functions is necessary to prevent unnecessary overlapping or spare filling of the memory size. So, we give it as a function of the number of items and the memory size:\n",
    "$$ k = \\frac{m}{n} ln(2)$$\n",
    "Depending on the number of items to be stored and an acceptable false positive rate, we can select the appropriate memory size given as:\n",
    "$$ m = -\\frac{n \\times ln(P)}{(ln(2))^2}$$\n",
    "Here, 'P' is the false positive probability. As we decrease the probability, we can see that memory size increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we want from counting bloom filters?\n",
    "The reason we use counting bloom filters is to see if each of the words that is inputted is in the initialized dictionary. In other words, we are checking if the word has been correctly spelled. If the word is correctly spelled, no more steps needs to be taken and we can simply return the word without any modifications. In our implementation, it will return $True$. But, if the word is not found, this guarantees that the word is misspelled because false negatives are not seen in counting bloom filters. So, we return $False$ and further processing occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingBloomFilter(object):\n",
    "    #We pass in the false positive rate and the number of items that needs be stored as the parameter. This is because the optimal number of hash functions and the array size can be computed by using the formula if we  have these parameters.\n",
    "    def __init__(self, fpr, num_item, seed, threshold):\n",
    "        self.threshold = threshold\n",
    "        self.seed = seed\n",
    "        self.num_item = num_item\n",
    "        self.fpr = fpr #I'm assuming that  fpr is given as a probability. If not, we can easily convert percentage into probability.\n",
    "        \n",
    "        #We will use the formula that we found above to find the size of the array and the number of hash functions.\n",
    "        #First for memory_size, because we need it to find the num_hashfn.\n",
    "        #Formula: m = -n ln(P)/(ln2)^2\n",
    "        self.memory_size = abs(int(round(-self.num_item * math.log(self.fpr))/(math.log(2)**2))) #For this we need the math library\n",
    "    \n",
    "        #Now, for number of hash functions\n",
    "        #Formula: k = (m/n)ln(2)\n",
    "        self.num_hashfn = abs(int(round((self.memory_size/self.num_item)* math.log(2))))\n",
    "        \n",
    "        #We have the size of the list, so we can create the list and initialize it with all zeros.\n",
    "        self.slots = self.memory_size * [0]\n",
    "        \n",
    "    def hash_cbf(self, item):\n",
    "        #We use the mmh3 hash to get the hash function. mmh3 has two arguments, the first is the item which is fixed, but we differe the seed.\n",
    "        for i in self.seed[:self.num_hashfn]:\n",
    "            self.hash_val = mmh3.hash(item, i) % self.memory_size\n",
    "        return self\n",
    "    \n",
    "    def search(self, item):\n",
    "        #Seed should give us the same values as long as the initial state are the same.\n",
    "        #We check if the slot value is greater than the threshold. For example: if a slot has value '2', then it compares it with the threshold we have set and return true if it is greater.\n",
    "        for seed_val in self.seed[:self.num_hashfn]:\n",
    "            if (self.slots[mmh3.hash(item, seed_val)%self.memory_size]) > self.threshold: #For this Assignment, I have set all the threshold to be 0.\n",
    "                pass\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def insert(self, item):\n",
    "        for seed_val in self.seed[:self.num_hashfn]:\n",
    "            #We get the index from the hash function, and we increase that position's value by 1.\n",
    "            self.slots[mmh3.hash(item, seed_val)%self.memory_size] = (self.slots[mmh3.hash(item, seed_val)%self.memory_size]) + 1\n",
    "        return self\n",
    "    \n",
    "    def delete(self, item):\n",
    "        #Searches if the item exists. If not, there is no way to delete it.\n",
    "        if self.search(item) == True:\n",
    "            for seed_val in self.seed[:self.num_hashfn]:\n",
    "                #Similar to when inserting, the value is incremented by 1. While deleting, we decrease this value by 1.\n",
    "                self.slots[mmh3.hash(item, seed_val)%self.memory_size] = (self.slots[mmh3.hash(item, seed_val)%self.memory_size]) - 1\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest Common Subsequence (LCS)\n",
    "Our first implementation consists the use of the longest common subsequence. We will compare the longest common subsequence of the misspelled word and each of the words in our dictionary. The combination with the longest common subsequence will be the correct word. However, there are a few flaws with this assumption. Firstly, when we have thousands of words, there are multiple combinations which have the same longest common subsequence. So, our goal must be to reduce the number of combinations that give the same LCS. \n",
    "\n",
    "For this, a few assumptions can be made. First is that the first letter of the misspelled word is always correct. We are not bothered with other letters of the word, but the first letter is always correct. This will be one filter which will help increase accuracy.\n",
    "\n",
    "Since we know that this problem satisfies both the properties required to be solved by dynamic programming, we can use dynamic programming to improve the efficiency.\n",
    "1. Optimal Substructure: We can break down the problem into smaller parts that are subproblems by themselves. We can do this until the solution becomes trivial.\n",
    "2. Overlapping subproblems: The solutions to the subproblems can be reused to solve other problems. We are using a LCS matrix to store these values so they don't have to be constantly recomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_common_subsequence(x, y): #Takes in the two strings to find the LCS of\n",
    "    #Lengths of the two strings\n",
    "    m = len(x)\n",
    "    n = len(y)\n",
    "    \n",
    "    #Initialize the tables of size mxn with all zeros.\n",
    "    tab = np.zeros((m, n))\n",
    "    \n",
    "    #If both the strings are null, return zero.\n",
    "    if m == 0 or n == 0: \n",
    "        return 0\n",
    "    \n",
    "    #If the first character of the strings match (Base Case)\n",
    "    if x[0] == y[0]:\n",
    "        tab[0, 0] = 1\n",
    "    \n",
    "    #1st column: Helps us find the character in x that matches the first one in y.\n",
    "    for i in range(1, m):\n",
    "        if tab[i - 1, 0]:\n",
    "            tab[i, 0] = 1\n",
    "        else:\n",
    "            if x[i] == y[0]:\n",
    "                tab[i, 0] = 1\n",
    "                \n",
    "    #1st row: Helps us find the character in y that matches the first one in x.\n",
    "    for j in range(1, n):\n",
    "        if tab[0, j - 1]:\n",
    "            tab[0, j] = 1\n",
    "        else:\n",
    "            if x[0] == y[j]:\n",
    "                tab[0, j] = 1\n",
    "    \n",
    "    #Apply the optimal substructure of the LCS problem.\n",
    "    for i in range(1, m): #Each character in string 1\n",
    "        for j in range(1, n): #Each character in string 2\n",
    "            #If the two characters match, set the diagonals to +1\n",
    "            if x[i] == y[j]:\n",
    "                tab[i, j] = tab[i - 1, j - 1] + 1\n",
    "            #If the two characters match, set the upper ones to +1\n",
    "            elif tab[i - 1, j] >= tab[i, j - 1]:\n",
    "                tab[i, j] = tab[i - 1, j]\n",
    "            else: \n",
    "                tab[i, j] = tab[i, j - 1]\n",
    "    \n",
    "    return tab[-1,-1] #Return the last value from the table i.e. the longest common subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will apply another filter that can help us improve even further upon the accuracy. This was done because words like 'thesee' were being corrected as 'tennessee', where the difference between the length of the two words are more than with 'these'. So, we will only consider those words that are within a certain range of the initial length of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_word(search_word, correct_words, thres): #This function returns the corrected word\n",
    "    checker_text = filtered_words(correct_words) #Filters the words from the dictionary.\n",
    "    typo_word = filtered_words(search_word) #This will filter the inputted word\n",
    "    len_max = 0 #This stores the value of the maximum LCS\n",
    "    word_index = 0 #This will keep track of the index for which the LCS was maximum\n",
    "    for i in range(len(checker_text)): #We will go through each of the words.\n",
    "        #Filter 1: If the first letter of both the words are the same, we move forward\n",
    "        if checker_text[i][0] == typo_word[0][0]:\n",
    "            #Finds the LCS from the two words\n",
    "            lo = longest_common_subsequence(checker_text[i], typo_word[0])\n",
    "            #If the new LCS is greater than the previous max,and the threshold is met, we update the values\n",
    "            if len_max < lo and abs(len(checker_text[i]) - len(typo_word[0]))<= (thres * len(typo_word[0])):\n",
    "                word_index = i\n",
    "                len_max = lo\n",
    "\n",
    "    return (checker_text[word_index]) #Returns the corrected word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the function to check if a word exists in the dictionary.\n",
    "def if_exists(search_word, correct_words):\n",
    "    l = filtered_words(correct_words)#We add one element to the list.\n",
    "    seed_list = [3,4,5,6,7,1] #Set the seeds.\n",
    "    cbf = CountingBloomFilter(0.0001,len(texts), seed_list,0) #Create an object with the given parameters\n",
    "    for i in l: # Goes through the list 'l'.\n",
    "        cbf.insert(i) #Insert the words inside the CBF.\n",
    "    if cbf.search(search_word):\n",
    "        return True #Searches for the word and if found return True\n",
    "    else:\n",
    "        return False #If not found, return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect1(test_word, texts, thres):\n",
    "    #If the word exists, we simply return the word\n",
    "    if (if_exists(test_word, texts)) == True:\n",
    "        return test_word\n",
    "    #If we don't find the word, we call the function corrected_word\n",
    "    elif (if_exists(test_word, texts)) == False:\n",
    "        return corrected_word(test_word, texts, thres)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_1(mispell):\n",
    "    thres = 0.3\n",
    "    mispelled_ = filtered_words(mispell)\n",
    "    corrected = []\n",
    "    for i in mispelled_:\n",
    "        corrected.append(autocorrect1(i, texts, thres))\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'that', 'these', 'times']\n"
     ]
    }
   ],
   "source": [
    "#Test cell\n",
    "test_line = \"helo worl , thhat thesee timess \"\n",
    "filtered_line = filtered_words(test_line)\n",
    "corrected_line = []\n",
    "thres = 0.3\n",
    "for i in filtered_line:\n",
    "    corrected_line.append(autocorrect1(i,texts, thres))\n",
    "print (corrected_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Distance\n",
    "The second approach will consider the edit distance between two strings. First, let's understand what edit distance encompasses. It is a measure that gives us the difference between two strings. \n",
    "Even in this case, we will not use the brute force approach i.e. find out all the strings whose edit distance is 1,2,... and terminate when the other string is achieved. This is incredibly inefficient with a time complexity of $O(2^n)$.\n",
    "\n",
    "The most popular edit distance algorithm is the Levenshtein Distance which uses three operations i.e. insertion, deletion and substitution to find the minimum number of edits we can make to get from one string to another. We will be using dynamic programming in this case since it satisfies both the properties. \n",
    "1. Overlapping subproblems : Just like with finding LCS we can use  table to store the minimum edit distances. So, when we need to find the new edit distance, we can find the previous minimum edit distance and decide on the optimal operations.\n",
    "2. Optimal substructure: We can break the problem into smaller subproblems and use recursion to find the final result.\n",
    "\n",
    "I will show one example on how this works. Let's say that the misspelled word is 'breaken' and the correct word is 'broken'. The steps would be:\n",
    "* breaken $\\rightarrow$ broaken (Substituting 'e' with 'o')\n",
    "* broaken $\\rightarrow$ broken (Delete 'a')\n",
    "\n",
    "But, in this implementation we will be using a slightly modified version of Levenshtein Distance called Damerau_Levenshtein Distance. The difference is that this method also considers tranposes i.e. swapping of the letter.\n",
    "\n",
    "We will be choosing the words that are closest to one another i.e. have the smallest edit distance. We don't want to increase the edit distance to be too high since it will substantially increase the possible operations increasing the time complexity. So, for this purpose, we will be looking at edit distance of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 'counter' as a data structure. We could have used Python dictionary, but there are some useful methods in 'counter'. It can tell us how many times we have seen the word in the initial dictionary. We will be using this data to get information about probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'hello'],\n",
       " ['h', 'ello'],\n",
       " ['he', 'llo'],\n",
       " ['hel', 'lo'],\n",
       " ['hell', 'o'],\n",
       " ['hello', '']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This funcion fetches all the possible combinations of the given word\n",
    "def possible_breaks(test_word):\n",
    "    li = []\n",
    "    for i in range (len(test_word)+1):\n",
    "        li.append([test_word[:i], test_word[i:]]) #Appends the possible combination to li.\n",
    "    return li\n",
    "possible_breaks('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_1(test_word): #Gives us a set of all possible 1 distance edits\n",
    "    all_letters = 'abcdefghijklmnopqrstuvwxyz' #For adding letters to the word.\n",
    "    removals = [] #List of words after removing one letter from the word\n",
    "    swap = [] #for transposes\n",
    "    insert = [] #For insertions\n",
    "    subs = []  #For substitutions\n",
    "    comb_s = possible_breaks(test_word) #Possible splits of the word\n",
    "    for (i,j) in comb_s: #Run until the second element of the list is empty.\n",
    "        if j:\n",
    "            removals=removals+([i+j[1:]]) #Removes the letter sequentially.\n",
    "    for (i,j) in comb_s: #Similar to above\n",
    "        if len(j)>1: #We cannnot swap one letter so it must be more than 1.\n",
    "            swap=swap+([i+j[1]+j[0]+j[2:]]) #Swap each combination.\n",
    "    for (i,j) in comb_s:\n",
    "        for k in all_letters: #Add each letter in all position\n",
    "            insert=insert+([i+k+j])\n",
    "    for (i,j) in comb_s:\n",
    "        for k in all_letters:\n",
    "            if j:\n",
    "                subs=subs+[i+k+j[1:]] #Substitude each of the letter with the alphabets\n",
    "    return set(removals+swap+subs+insert)#Gets rid of all duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter #Import the module counter\n",
    "words = filtered_words(text) #Filter the text\n",
    "occur = Counter(words) #Convert it into counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e', 'h', 't'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_(words):\n",
    "    return {i for i in words if i in occur} #Returns the set of words that are in the initial dictionary.\n",
    "correct_('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dist_0(test_word): #All set of words that are zero edits away from the initial word\n",
    "    return {test_word} #Returns the word itself\n",
    "dist_0('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_2(test_word):#All set of words that are two edits away from the initial word\n",
    "    d2 = [] \n",
    "    #One loop iterates through the list of words from dist_1 and other from the sublist of dist_1\n",
    "    for i in dist_1(test_word):\n",
    "        for j in dist_1(i):\n",
    "            d2.append(j) #Adds all the possible edits to the list\n",
    "    return set(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_chosen_one(test_word):\n",
    "    #Finds the correct word from the list\n",
    "    c = (correct_(dist_0(test_word)) or correct_(dist_1(test_word)) or correct_(dist_2(test_word)) or [test_word])\n",
    "    return max(c, key=occur.get), c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's understand how we weight our final choices using the number of times the word is seen in the dictionary text. The counter gives us the number of times each word is seen in the text, so we leverage that to get to the optimal answer. The example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road occurs  1 times\n",
      "does occurs  10 times\n",
      "dash occurs  5 times\n",
      "coast occurs  43 times\n",
      "dead occurs  16 times\n",
      "boast occurs  1 times\n",
      "dogs occurs  1 times\n",
      "doesn occurs  4 times\n",
      "load occurs  2 times\n",
      "dost occurs  1 times\n",
      "board occurs  58 times\n"
     ]
    }
   ],
   "source": [
    "#This gives us the top choice that we obtained.\n",
    "k = the_chosen_one('doasd')[1]\n",
    "for i in k:\n",
    "    print (i, 'occurs ', occur[i], 'times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code given above, we can see how many times the word is seen in the initial text. So, we choose the one that occurs the most number of times. In this case, it would be 'board'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This would be the suggestion the user would get.\n",
    "the_chosen_one('doasd')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_2(mispell):\n",
    "    mispelled_ = filtered_words(mispell)\n",
    "    corrected = []\n",
    "    for i in mispelled_:\n",
    "        corrected.append(the_chosen_one(i)[0])\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'welcome', 'to', 'earth']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocorrect_1('helloo worlnd, wlcome to eartyh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'welcome', 'to', 'party']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocorrect_2('helloo worlnd, wlcome to eartyy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you mean:  ['head']\n",
      "Suggestions:\n",
      "head\n",
      "{'heads', 'head', 'held', 'heed'}\n"
     ]
    }
   ],
   "source": [
    "search_word = 'hesdu'\n",
    "print (\"Do you mean: \", autocorrect_2(search_word))\n",
    "print ('Suggestions:')\n",
    "suggestions = ''\n",
    "for i in (the_chosen_one(search_word)):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "#### LCS and CBFs:\n",
    "This concludes our implementation of the spell checker using two distinct methods. The first method consisted of using the longest common subsequence to give the closest suggestion. This method used counting bloom filters as the data structure which can perform insertions and lookup in constant time. However, we also had to find the longest common subsequence between the misspelled word and every other word in the dictionary. If there are a total of 'n' words in the dictionary, and the length of each word is given as 'm', the worst case time complexity would be $O(nm^2)$. There are also other operations like reading the data and filtering it which takes $O(n)$ and $O(1)$ respectively.\n",
    "\n",
    "This implementation also covers some extra filters like stabilizing the first letter and having a threshold on the difference between the length of the words. So, this improves the accuracy substantially. We can also use a shorter initial dictionary, but it is much slower than the second approach. The correct word dictionary for this approach has around 75000 words compared to the other one at 425000 words. Trying to use the other dictionary was very slow and could not improve the accuracy since no account is made for the frequency of the words.\n",
    "\n",
    "#### Counters and Levenshtein Distance:\n",
    "The data structure that we use i.e. counter is a subclass of dictionary so the time complexity is the same as dictionary i.e. $O(n)$ since it has to go through each of the inputs. Then, the levenshtein distance takes $O(mn)$ to find the minimum edit distance, where 'm' and 'n' are the length of the strings. However, we need to iterate it over the entire counter. So, if we consider 'x' to be the length of the counter, the time complexity would be $O(mnx)$.\n",
    "\n",
    "The biggest pro of this method is the ability to get suggestions rather than only the final word. Since, we are weighting the words based on their frequency of occurrence, the even if the final answer is wrong, other suggestions should be very close. Another pro is the use of counters from which we could further find the probabilities of each words, and even phrases. This is not possible with the first approach. The time complexity is also better than the first approach, and the accuracy is also much greater."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
